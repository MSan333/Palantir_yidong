# 📘 数据层能力详解 - 从连接到治理

> **完整的数据工程技术栈** - Data Connection → Pipeline → SQL → Code Repositories

**对应工具编号**：03, 02, 04, 05
**适合角色**：数据工程师、数据架构师、BI开发者
**学习周期**：5-10天

---

## 📑 目录

1. [第1章：Data Connection - 多源接入](#第1章-data-connection---多源接入)
2. [第2章：Pipeline Builder - 可视化 ETL](#第2章-pipeline-builder---可视化etl)
3. [第3章：SQL 表达式 - 高阶查询](#第3章-sql-表达式---高阶查询)
4. [第4章：Code Repositories - 生产级开发](#第4章-code-repositories---生产级开发)
5. [性能优化指南](#性能优化指南)
6. [最佳实践与避坑](#最佳实践与避坑)

---

## 第1章：Data Connection - 多源接入

### 1.1 核心概念

**Data Connection** 是 Foundry 的**企业级数据集成引擎**，支持：

- **200+企业连接器**（数据库、API、云平台、应用系统）
- **三种同步模式**（快照、增量、流式）
- **多层级安全**（密钥管理、SSL加密、行级安全）
- **自动版本控制**（变更追踪、回滚能力）

### 1.2 支持的数据源类型

#### 【类型1】关系数据库

```
主流数据库：
├─ 传统：Oracle, SQL Server, PostgreSQL, MySQL
├─ 云数据仓库：Snowflake, Redshift, BigQuery, Databricks
└─ 国产：达梦, 人大金仓, TiDB

连接步骤：
1. Data Connection → 创建 Source
2. 选择 Connector 类型（如 Snowflake）
3. 输入连接字符串 (host, port, db, user)
4. 从密钥管理器获取密码（不直接保存）
5. 配置 Sync 规则：
   ├─ 快照 (Snapshot)：完整复制，每日/每周运行
   ├─ 增量 (Incremental)：仅同步变化，基于时间戳/序列号
   └─ CDC (Change Data Capture)：实时流式，延迟<1秒
6. 测试连接 → 部署 → 自动化运行
```

#### 【类型2】非结构化数据源

```
文件系统：
├─ S3（AWS）：支持分区、压缩、流式读取
├─ ADLS（Azure）：与 Databricks 无缝集成
├─ GCS（Google Cloud）：支持行级安全
└─ 本地文件共享：NFS, HDFS

API 数据源：
├─ REST API：支持分页、请求头、速率限制
├─ GraphQL：自动处理复杂查询
├─ Webhook：事件驱动的数据推送
└─ SFTP：安全文件传输

应用系统：
├─ SaaS：Salesforce, ServiceNow, HubSpot
├─ 金融系统：SAP, Oracle ERP, 用友
├─ 电商平台：淘宝、京东 API
└─ 社交媒体：Facebook, Twitter（历史数据）
```

### 1.3 实战案例1：连接 Snowflake 数据仓库

**业务背景**：零售企业需要接入 Snowflake 中的销售数据

```
【第1步】创建 Data Source
├─ 登录 Foundry → Data Connection
├─ 点击 "Create Data Source"
├─ 选择 Connector："Snowflake"
└─ 填入信息：
   ├─ Account: xy12345.us-east-1
   ├─ Warehouse: COMPUTE_WH
   ├─ Database: SALES_DB
   ├─ Schema: RAW
   └─ User: foundry_user

【第2步】配置认证
├─ Password 方式：
│  ├─ 创建密钥对象（Key-Value Store）
│  ├─ 存储 Snowflake 密码
│  └─ Data Connection 引用此密钥
├─ IP 白名单：
│  ├─ Foundry IP 段加入 Snowflake 防火墙
│  └─ 测试连接畅通
└─ SSL/TLS：自动启用

【第3步】配置表同步
├─ 选择同步表：
│  ├─ CUSTOMERS（客户表）
│  ├─ ORDERS（订单表）
│  ├─ ORDER_ITEMS（订单明细）
│  └─ PRODUCTS（产品表）
├─ 同步模式选择：
│  ├─ CUSTOMERS：快照模式（静态维度表）
│  │  └─ 每日 00:00 UTC 运行
│  ├─ ORDERS：增量模式（快速增长的事实表）
│  │  ├─ 时间戳列：UPDATED_AT
│  │  ├─ 首次同步：全量
│  │  └─ 后续同步：仅 UPDATED_AT > 上次时间戳
│  └─ ORDER_ITEMS：增量模式
│     └─ 同 ORDERS
└─ 配置数据转换（可选）：
   ├─ 列映射：Snowflake 列名 → Foundry 列名
   ├─ 过滤条件：WHERE UPDATED_AT > '2024-01-01'
   └─ 类型转换：TIMESTAMP → DateTime

【第4步】设置安全与权限
├─ 数据集权限：
│  ├─ CUSTOMERS：仅"销售团队"可查
│  ├─ ORDERS：全公司可查
│  └─ PRODUCTS：全公司可查
├─ 行级安全（RLS）：
│  ├─ ORDERS 按地区过滤
│  │  └─ 北区销售 仅看"北区"订单
│  └─ CUSTOMERS 按客户经理过滤
├─ 列级安全：
│  ├─ 隐藏敏感列（CREDIT_CARD）
│  └─ 仅管理员可见
└─ 审计日志：
   └─ 记录所有数据访问（合规要求）

【第5步】启动同步并监控
├─ 点击"Enable Sync"激活
├─ 初次同步：
│  ├─ CUSTOMERS (5M 行)：2 分钟
│  ├─ ORDERS (50M 行)：8 分钟
│  ├─ ORDER_ITEMS (150M 行)：25 分钟
│  └─ PRODUCTS (100k 行)：30 秒
├─ 持续监控：
│  ├─ Builds 应用 → 选择此 Data Source
│  ├─ 查看历史同步记录
│  ├─ 每次同步的成功/失败状态
│  └─ 数据行数变化趋势
└─ 异常告警：
   ├─ 设置告警规则
   │  └─ 同步失败 → 发送 Slack 通知
   ├─ 数据量异常 → 告警
   └─ 同步耗时超过 1 小时 → 告警

【实际效果】

数据源配置完成后，Foundry 中即可看到：
┌─────────────────────────────────┐
│ 📦 Data Connection Status       │
├─────────────────────────────────┤
│ Source: Snowflake Sales DB      │
│ Status: ✅ Connected            │
│ Last Sync: 2024-01-16 08:00 UTC │
│ Next Sync: 2024-01-16 20:00 UTC │
│                                 │
│ Tables Synced: 4                │
│ ├─ CUSTOMERS: 2.1M rows         │
│ ├─ ORDERS: 52.3M rows           │
│ ├─ ORDER_ITEMS: 156.8M rows     │
│ └─ PRODUCTS: 105k rows          │
│                                 │
│ Total Data Size: 142 GB         │
│ Sync Frequency: Daily (00:00)   │
│ Next Actions: [View Data] [Edit]│
└─────────────────────────────────┘

预期时间：30分钟-1小时
```

### 1.4 实战案例2：接入 REST API（如客户订阅数据）

**业务背景**：需要定期拉取第三方 SaaS 应用（如 Stripe）的订阅数据

```
【第1步】配置 REST API Connector
├─ 创建 Data Source → 选择 "REST API"
├─ 基础配置：
│  ├─ API 端点：https://api.stripe.com/v1/subscriptions
│  ├─ 认证方式：Bearer Token
│  │  ├─ 从密钥管理器获取 Stripe API Key
│  │  └─ 自动添加 Authorization header
│  ├─ 请求头配置：
│  │  ├─ Content-Type: application/json
│  │  └─ User-Agent: Foundry/1.0
│  └─ 超时设置：30秒

【第2步】配置分页与增量同步
├─ 分页方式识别（因 API 而异）：
│  ├─ Stripe 使用 cursor 分页
│  ├─ 配置参数：
│  │  ├─ Page Token 字段：starting_after
│  │  ├─ 每页行数：100
│  │  └─ 停止条件：has_more = false
│  └─ 系统自动遍历所有页
├─ 增量同步配置：
│  ├─ 时间戳字段：created
│  ├─ 查询参数：?created[gte]={last_sync_time}
│  └─ 确保只拉取新/更新的订阅
└─ 处理速率限制：
   ├─ Stripe 限制：100 请求/秒
   ├─ Foundry 自动：
   │  ├─ 限流：遵守 API 速率限制
   │  ├─ 重试：失败后指数退避
   │  └─ 监控：告警异常请求

【第3步】数据变换与清洗
├─ 响应结构通常是嵌套的：
│  ├─ 原始响应：{ data: [{...}, {...}], ... }
│  ├─ 自动提取数组：data 字段
│  └─ 扁平化嵌套对象
├─ 字段映射：
│  ├─ id → subscription_id
│  ├─ customer.id → customer_id
│  ├─ amount → subscription_amount
│  ├─ billing_cycle_anchor → next_billing_date
│  └─ status → subscription_status
├─ 类型转换：
│  ├─ 时间戳 (Unix) → DateTime
│  ├─ 金额 (美分) → 美元 (除以100)
│  └─ 布尔值 → Boolean
└─ 脱敏处理（可选）：
   ├─ 隐藏完整 API 密钥
   └─ 记录仅用户ID (不含完整卡号)

【第4步】启动同步
├─ 初次同步：
│  ├─ Stripe: 所有 2.5M 订阅
│  ├─ 耗时：约 5 分钟（依赖 API 速率）
│  └─ 生成数据集：stripe_subscriptions
├─ 增量同步（每小时）：
│  ├─ 仅拉取过去 1 小时内的新/变更订阅
│  ├─ 通常 <1 分钟 完成
│  └─ 高效更新
└─ 错误处理：
   ├─ API 故障时：重试最多 3 次
   ├─ 若仍失败：发送告警，延迟至下次运行
   └─ 数据一致性保证：至少一次交付

【实际效果】

Foundry 中可查询完整的订阅数据：

SELECT
  subscription_id,
  customer_id,
  subscription_amount,
  subscription_status,
  next_billing_date
FROM stripe_subscriptions
WHERE subscription_status = 'active'
LIMIT 10

预期时间：20分钟-1小时
```

### 1.5 关键特性总结

| 特性 | 说明 | 适用场景 |
|------|------|--------|
| **即插即用连接器** | 200+ 预构建的连接器 | 无需编码，快速集成 |
| **多种同步模式** | 快照/增量/CDC | 灵活应对不同数据特性 |
| **自动版本控制** | 数据变更历史追踪 | 审计、回滚、数据治理 |
| **加密与认证** | SSL、密钥管理、密码脱敏 | 企业级安全 |
| **错误恢复** | 自动重试、告警通知 | 高可用性 |
| **行列级安全** | RLS、列脱敏、用户权限 | 合规与隐私保护 |
| **监控与告警** | 同步状态、性能指标 | 运维支持 |

---

## 第2章：Pipeline Builder - 可视化 ETL

### 2.1 核心概念

**Pipeline Builder** 是 Foundry 的**可视化 ETL 工程平台**：

- **无代码画布**：拖拖拽拽构建数据流
- **200+转换组件**：过滤、联接、聚合、透视等
- **Git 版本控制**：所有 Pipeline 代码自动版本化
- **参数化与复用**：构建可复用的数据处理模块

### 2.2 Pipeline 的基本结构

```
┌─────────────┐
│ 数据源节点   │  ← 输入 (Data Source, S3, API 等)
│ (Inputs)    │
└──────┬──────┘
       │
       ▼
┌──────────────────────┐
│  转换层 (Transform)  │  ← 核心 ETL 逻辑
│  ├─ Filter           │
│  ├─ Join             │
│  ├─ Aggregate        │
│  ├─ Pivot            │
│  └─ Custom SQL       │
└──────────────────────┘
       │
       ▼
┌─────────────┐
│ 输出节点     │  ← 目标数据集 (输出)
│ (Outputs)   │
└─────────────┘
```

### 2.3 实战案例1：构建销售数据 ETL 管道

**业务背景**：从多个数据源构建统一的订单数据集

```
【构建过程】

【第1步】创建 Pipeline
├─ Foundry → Pipeline Builder
├─ 点击 "Create New Pipeline"
├─ 输入名称："order_processing"
├─ 选择分支："feature/order-etl"（不直接修改 Main）
└─ 进入编辑界面

【第2步】添加数据源节点
├─ 点击"Add Input"添加第一个数据源
│  ├─ 输入名称："orders_raw"
│  ├─ 选择数据源：Snowflake 中的 ORDERS 表
│  ├─ 字段映射：
│  │  ├─ order_id → order_id
│  │  ├─ customer_id → customer_id
│  │  ├─ order_date → order_date
│  │  ├─ order_amount → order_amount
│  │  └─ status → order_status
│  └─ 行级过滤（可选）：
│     └─ WHERE order_date >= '2024-01-01'
│
├─ 点击"Add Input"添加第二个数据源
│  ├─ 输入名称："customers"
│  ├─ 选择数据源：Snowflake 中的 CUSTOMERS 表
│  └─ 字段：customer_id, customer_name, region, segment
│
└─ 点击"Add Input"添加第三个数据源
   ├─ 输入名称："products"
   ├─ 选择数据源：Snowflake 中的 PRODUCTS 表
   └─ 字段：product_id, product_name, category, price

【第3步】添加转换节点
├─ 转换1：Join orders 与 customers
│  ├─ 从"orders_raw"拖线到"Join"组件
│  ├─ 从"customers"拖线到"Join"组件
│  ├─ 配置 Join：
│  │  ├─ Join 类型：LEFT JOIN（保留所有订单）
│  │  ├─ 关键字：orders_raw.customer_id = customers.customer_id
│  │  └─ 输出字段：
│  │     ├─ order_id, order_date, order_amount, order_status
│  │     ├─ customer_name, region, segment
│  │     └─ customer_id
│  └─ 输出命名："orders_with_customers"
│
├─ 转换2：再次 Join 与 products（通过订单项）
│  ├─ 注意：orders_raw 可能有多行（一个订单多个产品）
│  ├─ 需先从 ORDER_ITEMS 添加
│  │  └─ 添加新 Input："order_items" (oi.order_id, oi.product_id, oi.qty)
│  ├─ 再 Join：order_items LEFT JOIN products
│  │  ├─ 关键字：order_items.product_id = products.product_id
│  │  └─ 输出：qty, product_name, category, price
│  └─ 输出命名："orders_with_products"
│
├─ 转换3：聚合与计算
│  ├─ 从"orders_with_products"添加"Aggregate"组件
│  ├─ 分组维度：order_id, customer_id, customer_name, region, order_date
│  ├─ 聚合指标：
│  │  ├─ SUM(qty) as total_quantity
│  │  ├─ SUM(qty * price) as order_total
│  │  ├─ COUNT(DISTINCT product_id) as num_products
│  │  └─ ARRAY_AGG(product_name) as products_list
│  └─ 输出命名："orders_aggregated"
│
├─ 转换4：添加派生列
│  ├─ 从"orders_aggregated"添加"Custom SQL"组件
│  ├─ 编写 SQL：
│  │  ├─ order_priority = CASE
│  │  │                    WHEN order_total > 10000 THEN 'HIGH'
│  │  │                    WHEN order_total > 1000 THEN 'MEDIUM'
│  │  │                    ELSE 'LOW'
│  │  │                   END
│  │  ├─ order_day_of_week = DAYOFWEEK(order_date)
│  │  ├─ order_is_weekend = order_day_of_week IN (6, 7)
│  │  └─ order_season = CASE
│  │                     WHEN MONTH(order_date) IN (12,1,2) THEN 'Winter'
│  │                     WHEN MONTH(order_date) IN (3,4,5) THEN 'Spring'
│  │                     WHEN MONTH(order_date) IN (6,7,8) THEN 'Summer'
│  │                     ELSE 'Fall'
│  │                    END
│  └─ 输出命名："orders_with_derived_cols"
│
└─ 转换5：数据质量检查与过滤
   ├─ 添加"Filter"组件
   ├─ 过滤条件：
   │  ├─ order_total > 0 (排除无效订单)
   │  ├─ order_date IS NOT NULL
   │  └─ customer_id IS NOT NULL
   └─ 输出命名："orders_cleaned"

【第4步】添加输出节点
├─ 从"orders_cleaned"添加"Output"组件
├─ 配置输出：
│  ├─ 输出名称："orders_final"
│  ├─ 输出类型：Ontology Object（本体对象）
│  │  └─ 选择本体中的"Order"对象类型
│  ├─ 字段映射：
│  │  ├─ order_id → id
│  │  ├─ customer_id → customer
│  │  ├─ order_date → createdAt
│  │  ├─ order_total → totalAmount
│  │  ├─ order_priority → priority
│  │  ├─ region → customerRegion
│  │  └─ products_list → products
│  └─ 更新策略：
│     ├─ 主键：order_id
│     ├─ 更新方式：UPSERT（有则更新，无则插入）
│     └─ 保留时间戳：createdAt 不更新

【第5步】配置参数化（为复用做准备）
├─ 点击"Parameters"面板
├─ 添加参数1：
│  ├─ 参数名：$start_date
│  ├─ 类型：DateTime
│  ├─ 默认值：CURRENT_DATE - 30 天
│  └─ 描述："处理的起始日期"
├─ 添加参数2：
│  ├─ 参数名：$min_order_amount
│  ├─ 类型：Double
│  ├─ 默认值：0
│  └─ 描述："最小订单金额"
└─ 在 Filter 中使用：
   └─ WHERE order_date >= $start_date AND order_total >= $min_order_amount

【第6步】测试并部署
├─ 点击"Run"按钮测试 Pipeline
│  ├─ 系统编译 Pipeline 为 Spark 代码
│  ├─ 在开发集群上运行（用小数据量）
│  ├─ 进度显示：正在处理... 50%
│  └─ 完成后显示：
│     ├─ ✅ Pipeline 成功运行
│     ├─ 输入行数：orders_raw (5.2M), customers (100k), order_items (15.8M)
│     ├─ 输出行数：orders_final (5.2M)
│     ├─ 耗时：2 分 30 秒
│     └─ 数据预览（前 100 行）
├─ 查看生成的代码：
│  ├─ Pipeline 自动生成 Python/PySpark 代码
│  ├─ 代码存储在 Git 仓库中
│  └─ 支持版本对比与历史
├─ 点击"Commit & Push"提交代码
│  ├─ 输入 Commit 消息："Add order processing ETL pipeline"
│  ├─ 选择目标分支："feature/order-etl"
│  └─ 自动创建 Pull Request
├─ 代码审查（可选但推荐）：
│  ├─ 邀请数据工程师 review
│  ├─ 提出修改意见 → 自动更新
│  └─ 审核通过后 Approve
└─ Merge 到 Main
   ├─ 自动构建生产版本
   ├─ Pipeline 在生产环境部署
   └─ 配置定期运行（如每日 02:00 UTC）

【实际效果】

Pipeline 运行完成后，数据模型中出现完整的订单数据：

Query in Contour:
SELECT
  order_id,
  customer_id,
  customer_name,
  order_date,
  order_total,
  order_priority,
  region
FROM orders_final
WHERE order_date >= '2024-01-01'
ORDER BY order_total DESC

结果：
┌──────────┬─────────────┬──────────────┬──────────────┐
│ order_id │ customer_id │ customer_name│ order_total  │
├──────────┼─────────────┼──────────────┼──────────────┤
│ ORD-0001 │ CUST-100    │ ACME Corp    │ $125,400.50  │
│ ORD-0002 │ CUST-200    │ TechCorp Inc │ $89,200.00   │
└──────────┴─────────────┴──────────────┴──────────────┘

预期时间：1-2 小时
```

### 2.4 常见转换组件详解

| 组件 | 功能 | 使用场景 | 示例 |
|------|------|--------|------|
| **Filter** | 行级过滤 | 排除无效数据 | WHERE status != 'cancelled' |
| **Join** | 表联接 | 合并多源数据 | LEFT JOIN customers ON ... |
| **Aggregate** | 分组聚合 | 统计分析 | GROUP BY region; SUM(amount) |
| **Pivot** | 行转列 | 生成交叉表 | 各月销售额交叉表 |
| **Custom SQL** | SQL 查询 | 复杂逻辑 | 窗口函数、复杂派生列 |
| **Deduplicate** | 去重 | 保证唯一性 | 按 ID 去重，保留最新 |
| **Union** | 纵向合并 | 合并同类数据 | 合并多个月份的数据 |
| **Split** | 条件拆分 | 分类处理 | 按优先级分为三个流 |

### 2.5 性能优化技巧

```
1. 早期过滤
   ✅ 好：在输入时过滤 → 减少后续处理数据量
   ❌ 坏：最后才过滤 → 浪费计算资源

2. 分区与并行
   ✅ 好：按地区/月份分区 → 提高并行度
   ❌ 坏：单一分区 → 单机处理

3. 避免大表 Join
   ✅ 好：JOIN 大表时，右表先 DISTINCT
   ❌ 坏：JOIN 两个大表 → 内存溢出

4. 使用物化表
   ✅ 好：Pipeline 输出保存为表 → 下游快速查询
   ❌ 坏：每次查询都重新计算 → 浪费资源

5. 参数化查询
   ✅ 好：使用参数 $date_range → 灵活复用
   ❌ 坏：硬编码日期 → 每次修改代码
```

---

## 第3章：SQL 表达式 - 高阶查询

### 3.1 核心概念

**SQL 表达式板** 允许在 Pipeline 或 Contour 中使用 **Spark SQL**，支持：

- **窗口函数**（ROW_NUMBER, RANK, LAG, LEAD 等）
- **复杂派生列**（CASE、IF、正则表达式等）
- **高阶聚合**（PERCENTILE、STDDEV、HISTOGRAM 等）
- **表达式库**（保存常用表达式供复用）

### 3.2 常用 SQL 表达式

```sql
-- 【示例1】窗口函数：排序与排名
-- 需求：为每个地区的订单按金额排序，并标记排名

SELECT
  order_id,
  customer_id,
  order_amount,
  region,
  -- 在每个地区内，按订单金额排序
  ROW_NUMBER() OVER (PARTITION BY region ORDER BY order_amount DESC) as rank_in_region,
  -- 计算排名（相同金额同排名）
  RANK() OVER (PARTITION BY region ORDER BY order_amount DESC) as rank_with_tie,
  -- 计算运行累积和
  SUM(order_amount) OVER (PARTITION BY region ORDER BY order_amount DESC
                          ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as cumulative_sum
FROM orders
ORDER BY region, rank_in_region;

-- 【示例2】时间序列分析
-- 需求：计算每个客户的月度收入及环比增长

SELECT
  customer_id,
  DATE_TRUNC('month', order_date) as month,
  SUM(order_amount) as monthly_revenue,
  -- 上月收入（环比）
  LAG(SUM(order_amount), 1, 0) OVER (
    PARTITION BY customer_id
    ORDER BY DATE_TRUNC('month', order_date)
  ) as prev_month_revenue,
  -- 计算环比增长率
  ROUND(
    (SUM(order_amount) - LAG(SUM(order_amount), 1, 0) OVER (
      PARTITION BY customer_id
      ORDER BY DATE_TRUNC('month', order_date)
    )) / LAG(SUM(order_amount), 1, 1) OVER (
      PARTITION BY customer_id
      ORDER BY DATE_TRUNC('month', order_date)
    ) * 100, 2
  ) as mom_growth_pct
FROM orders
WHERE customer_id IS NOT NULL
GROUP BY customer_id, DATE_TRUNC('month', order_date)
ORDER BY customer_id, month;

-- 【示例3】CASE 条件表达式
-- 需求：根据订单金额和地区，分配订单优先级和销售区域

SELECT
  order_id,
  order_amount,
  region,
  CASE
    WHEN order_amount > 100000 THEN 'VIP'
    WHEN order_amount > 50000 THEN 'PREMIUM'
    WHEN order_amount > 10000 THEN 'STANDARD'
    ELSE 'BASIC'
  END as customer_tier,
  CASE
    WHEN region IN ('北京', '上海', '深圳') THEN '一级城市'
    WHEN region IN ('杭州', '南京', '武汉') THEN '二级城市'
    ELSE '其他'
  END as region_level,
  -- 组合条件
  CASE
    WHEN order_amount > 50000 AND region = '北京' THEN 1  -- 最高优先级
    WHEN order_amount > 30000 AND region IN ('上海', '深圳') THEN 2
    WHEN order_amount > 10000 THEN 3
    ELSE 4
  END as priority
FROM orders;

-- 【示例4】字符串与正则表达式
-- 需求：从邮箱提取域名和公司名

SELECT
  customer_email,
  -- 提取域名部分
  SUBSTRING(customer_email FROM POSITION('@' IN customer_email) + 1) as email_domain,
  -- 检查是否为企业邮箱
  CASE
    WHEN customer_email LIKE '%@company.com' THEN '内部'
    WHEN customer_email LIKE '%@%.com' THEN '商业'
    WHEN customer_email LIKE '%@%.edu' THEN '教育'
    ELSE '其他'
  END as email_type,
  -- 正则表达式匹配
  CASE
    WHEN customer_email RLIKE '^[a-z0-9._%+-]+@[a-z0-9.-]+\.[a-z]{2,}$' THEN 'valid'
    ELSE 'invalid'
  END as email_validation
FROM customers;

-- 【示例5】聚合函数与统计
-- 需求：计算每个地区的销售统计

SELECT
  region,
  COUNT(*) as order_count,
  SUM(order_amount) as total_revenue,
  AVG(order_amount) as avg_order_value,
  MIN(order_amount) as min_order,
  MAX(order_amount) as max_order,
  -- 标准差（衡量订单金额波动）
  STDDEV_POP(order_amount) as revenue_stddev,
  -- 百分位数（如第 75 百分位）
  PERCENTILE_APPROX(order_amount, 0.75) as p75_revenue,
  -- 众数（最常见的金额范围）
  MODE(ROUND(order_amount / 1000) * 1000) as mode_revenue
FROM orders
GROUP BY region
ORDER BY total_revenue DESC;

-- 【示例6】JSON 数据处理
-- 需求：从嵌套的 JSON 字段中提取数据

SELECT
  order_id,
  -- 提取顶层字段
  metadata['customer_type'] as customer_type,
  metadata['payment_method'] as payment_method,
  -- 提取嵌套字段
  metadata['shipping']['address']['city'] as shipping_city,
  metadata['shipping']['address']['country'] as shipping_country,
  -- 转换为数组并遍历
  metadata['items'] as order_items
FROM orders_with_metadata;

-- 【示例7】关联数组聚合
-- 需求：每个客户的所有订单 ID 聚合为列表

SELECT
  customer_id,
  customer_name,
  COUNT(*) as order_count,
  -- 聚合为数组
  COLLECT_LIST(order_id) as order_ids,
  -- 聚合为逗号分隔字符串
  CONCAT_WS(',', COLLECT_LIST(product_name)) as products_purchased,
  -- 按 order_date 排序后聚合
  COLLECT_LIST(order_amount)
    FILTER (WHERE order_date >= '2024-01-01') as recent_orders
FROM orders
GROUP BY customer_id, customer_name;

-- 【示例8】窗口函数的实际应用
-- 需求：检测每个客户的连续购买天数

WITH customer_visits AS (
  SELECT
    customer_id,
    DATE(order_date) as visit_date,
    -- 标记连续天数（如果与前一天不连续则重新计数）
    ROW_NUMBER() OVER (
      PARTITION BY customer_id
      ORDER BY DATE(order_date)
    ) as visit_seq,
    -- 计算与前一次访问的天数差
    DATEDIFF(
      DATE(order_date),
      LAG(DATE(order_date), 1) OVER (
        PARTITION BY customer_id
        ORDER BY DATE(order_date)
      )
    ) as days_since_last_visit
  FROM orders
)
SELECT
  customer_id,
  visit_date,
  days_since_last_visit,
  -- 判断是否连续（差=1 表示连续）
  CASE
    WHEN days_since_last_visit = 1 OR days_since_last_visit IS NULL THEN 1
    ELSE 0
  END as is_consecutive
FROM customer_visits
ORDER BY customer_id, visit_date;
```

### 3.3 表达式库与复用

```
【第1步】创建表达式模板
├─ 常用表达式1：计算同环比增长
│  └─ (current_value - prev_period) / prev_period * 100
├─ 常用表达式2：客户分层
│  └─ CASE WHEN revenue > 100k THEN 'VIP' WHEN ...
├─ 常用表达式3：异常检测（超过 3 倍标准差）
│  └─ ABS(value - mean) > 3 * stddev
└─ 常用表达式4：日期分组（按周）
   └─ DATE_TRUNC('week', order_date)

【第2步】在 Pipeline 中引用
├─ 编写 SQL 时，输入表达式名称
├─ 系统自动补全并插入
└─ 无需重复编写

【实际效果】
大幅减少重复代码，提高 ETL 开发效率
```

---

## 第4章：Code Repositories - 生产级开发

### 4.1 核心概念

**Code Repositories** 是 Foundry 的**生产级数据工程平台**，支持：

- **Python/PySpark 编程**
- **装饰器模式**（将函数转为数据流程）
- **Git 完整版本控制**
- **CI/CD 管道**（自动测试、部署）
- **依赖管理**（外部库、版本锁定）

### 4.2 基本架构

```python
# 【示例 1】简单的数据转换函数

from foundry import transform_df
import pyspark.sql.functions as F

@transform_df
def process_orders(ctx):
    """处理订单数据的变换函数"""
    # 读取输入数据集
    df = ctx.input("orders_raw").dataframe()

    # 数据转换逻辑
    processed = (
        df
        # 过滤无效订单
        .filter(F.col("order_amount") > 0)
        .filter(F.col("customer_id").isNotNull())
        # 添加派生列
        .withColumn(
            "order_year",
            F.year(F.col("order_date"))
        )
        .withColumn(
            "order_month",
            F.month(F.col("order_date"))
        )
        # 按地区分组统计
        .groupBy("region", "order_year", "order_month")
        .agg(
            F.count("*").alias("order_count"),
            F.sum("order_amount").alias("total_revenue"),
            F.avg("order_amount").alias("avg_order_value")
        )
    )

    # 返回结果（自动保存为输出数据集）
    return processed
```

```python
# 【示例 2】多输入多输出函数

@transform_df
def enrich_orders_with_customer_data(ctx):
    """用客户数据丰富订单数据"""
    # 读取多个输入
    orders_df = ctx.input("orders").dataframe()
    customers_df = ctx.input("customers").dataframe()
    products_df = ctx.input("products").dataframe()

    # JOIN 数据
    enriched = (
        orders_df
        .join(customers_df, "customer_id", "left")
        .join(products_df, "product_id", "left")
        # 添加客户等级
        .withColumn(
            "customer_tier",
            F.when(F.col("lifetime_value") > 100000, "VIP")
             .when(F.col("lifetime_value") > 50000, "Premium")
             .otherwise("Standard")
        )
    )

    # 输出多个数据集
    ctx.output(enriched, "orders_enriched")
    ctx.output(
        enriched.filter(F.col("customer_tier") == "VIP"),
        "vip_orders"
    )
```

### 4.3 高阶特性

```python
# 【示例 3】带参数的数据处理

@transform_df
def process_orders_with_parameters(ctx):
    """支持运行时参数的数据处理"""
    # 从上下文获取参数
    min_order_amount = ctx.parameter("min_order_amount", default=0.0)
    target_region = ctx.parameter("target_region", default="All")

    df = ctx.input("orders").dataframe()

    # 根据参数过滤
    if target_region != "All":
        df = df.filter(F.col("region") == target_region)

    result = df.filter(F.col("order_amount") >= min_order_amount)

    return result

# 调用时可指定参数：
# ctx.run(
#   function="process_orders_with_parameters",
#   parameters={
#     "min_order_amount": 1000.0,
#     "target_region": "北区"
#   }
# )
```

```python
# 【示例 4】关键工程实践

import logging

@transform_df
def production_grade_etl(ctx):
    """生产级 ETL 函数，包含日志、错误处理"""

    logger = logging.getLogger("order_processing")
    logger.info("开始处理订单数据")

    try:
        df = ctx.input("orders").dataframe()
        logger.info(f"读取 {df.count()} 行订单数据")

        # 数据质量检查
        null_check = df.filter(F.col("order_id").isNull()).count()
        if null_check > 0:
            logger.warning(f"发现 {null_check} 条 order_id 为空")
            # 根据业务规则决定是否继续
            # raise ValueError("存在不可接受的空值")

        # 主要处理逻辑
        result = (
            df
            .filter(F.col("order_amount") > 0)
            # ...更多转换...
        )

        logger.info(f"处理完成，输出 {result.count()} 行")
        return result

    except Exception as e:
        logger.error(f"处理失败: {str(e)}", exc_info=True)
        raise  # 重新抛出异常，触发告警
```

### 4.4 Git 工作流

```
1. 创建特性分支（本地开发）
   git checkout -b feature/order-processing

2. 编写与测试代码
   # 编辑 .py 文件
   # 本地运行测试

3. 提交代码
   git add .
   git commit -m "Add order processing ETL"

4. 推送并创建 Pull Request
   git push origin feature/order-processing
   # 在 Foundry UI 中创建 PR

5. 代码审查
   # 邀请同事 review
   # 解决 comments

6. Merge 到 Main
   # 自动触发构建与部署
   # Pipeline 在生产环境运行

7. 监控与告警
   # 查看执行结果
   # 设置性能告警
```

---

## 性能优化指南

### 优化1：数据分区

```sql
-- ❌ 不好：单一大表，全表扫描
SELECT * FROM orders WHERE order_date > '2024-01-01'

-- ✅ 好：表已按月分区
-- Foundry 自动只读取 >2024-01-01 的分区
-- 减少 I/O 成本 80%+
SELECT * FROM orders WHERE order_date > '2024-01-01'

-- 建议：在 Pipeline 输出时指定分区
-- 按 order_date 月份分区
-- 按 region 地区分区
```

### 优化2：物化与缓存

```
【需求】多个报告都需要计算"月度销售汇总"

❌ 不好做法：每个报告都运行相同的聚合
- 浪费计算资源
- 查询慢

✅ 好做法：创建物化表
1. 在 Pipeline 中构建"monthly_sales_summary"
2. 每月 01:00 自动运行
3. 报告直接查询物化表（<1秒）
4. 成本减少 10 倍
```

### 优化3：列式存储与压缩

```
默认配置已启用：
├─ Parquet 格式（列式存储）
│  └─ 相同列值连续存储 → 压缩率高
├─ Snappy 压缩
│  └─ 平衡压缩率与速度
└─ 谓词下推
   └─ 在扫描时就过滤，减少数据传输

结果：
- 同样数据，磁盘空间 ↓ 80%
- 查询速度 ↑ 2-5 倍
```

---

## 最佳实践与避坑

### 最佳实践

1. **参数化您的 Pipeline**
   - 用 `$date_range` 代替硬编码日期
   - 便于复用与时间序列处理

2. **版本控制与代码审查**
   - 所有 ETL 代码必须通过 PR 审核
   - 保证代码质量与文档

3. **监控与告警**
   - 配置数据量异常告警
   - 配置执行时间告警
   - 配置失败重试与通知

4. **数据质量检查**
   - Pipeline 中嵌入 DQ 检查
   - NULL 检查、重复检查、范围检查

5. **增量同步而非全量**
   - 使用增量模式 (Incremental/CDC)
   - 仅处理新/变更数据
   - 成本和速度都提升

### 常见错误与避坑

| 错误 | 症状 | 解决方案 |
|------|------|--------|
| **内存溢出** | Job 失败，无结果 | 添加 Filter 早期过滤；使用分区 |
| **JOIN 超时** | 查询卡住 | 检查 JOIN 条件是否为 NULL；使用广播 JOIN |
| **数据冷启动** | 首次 Sync 超级慢 | 可以接受；增量后会快很多 |
| **时间戳不对齐** | 增量 Sync 重复/漏掉数据 | 确保时间戳单调递增；使用 CDC 更安全 |

---

## 总结与下一步

### 关键要点

✅ **Data Connection**：轻松连接 200+ 企业数据源
✅ **Pipeline Builder**：无代码 ETL，快速构建
✅ **SQL 表达式**：高阶分析，复杂逻辑
✅ **Code Repositories**：生产级代码，完整版本控制

### 下一步学习

- 📗 **分析层**：如何查询与分析这些数据
- 📙 **应用层**：如何展示与操作这些数据
- 🏗️ **架构设计**：如何系统性地组织整个数据工作流

---



