在 Palantir Foundry 中，利用 **AIP Logic** 构建 **RAG（检索增强生成）** 工作流是一种将大型语言模型（LLM）与本体（Ontology）数据相结合的无代码方式，旨在生成**“植根于现实（Grounded in reality）”**且准确的 AI 回复 [1, 2]。

以下是构建 RAG 工作流的详细步骤与核心概念：

### 1. 核心原理：语义检索与本体
RAG 的核心在于“检索”，即在生成回复前，先从可靠的私有数据源中提取相关信息。
*   **语义搜索（Semantic Search）：** 相比传统的关键词匹配或 SQL 过滤，语义搜索允许基于**含义（Meaning）**查找最相关的结果 [3]。
*   **嵌入属性（Embedding Property）：** 为了实现语义搜索，本体中的对象类型必须包含一个嵌入属性。这是一种**向量表示（Vector representation）**，由 OpenAI Ada 等模型生成，能够捕捉字符串的深层语义 [4, 5]。

### 2. 创建 AIP Logic 函数与定义输入
*   **初始化：** 通过 Foundry 的快捷键 `Ctrl + J` 搜索并进入 **AIP Logic**，创建一个新的逻辑函数（如“主题公园游客接待员”）[6]。
*   **定义输入参数：** 为函数添加必要的输入（Function Inputs），例如**游客姓名（Guest Name）**、**人设（Persona）**以及**偏好（Preferences）** [7-9]。

### 3. 构建检索层：语义搜索块
这是 RAG 的第一步，负责从本体中获取上下文。
*   **配置搜索块：** 添加 **Semantic Search Block** [3]。
*   **选择语料库（Corpus）：** 选择包含嵌入属性的**对象集（Object Set）**（如“Ride”对象）作为搜索范围 [3, 5]。
*   **设置查询与数量：** 将用户的“偏好”输入作为查询语句，并指定返回的相关结果数量（例如返回 5 个结果） [5, 10]。

### 4. 数据格式化与上下文注入
检索到的数据需要被转化为 LLM 能够处理的格式：
*   **字符串转换：** 系统会自动创建一个块，将语义搜索返回的对象集**打印为字符串** [2, 11]。
*   **属性选择：** 你可以指定要在字符串中包含哪些属性（如名称和描述），这些信息将作为上下文喂给模型 [11]。

### 5. 构建生成层：使用 LLM 块
*   **配置指令：** 添加 **Use LLM Block** [7]。
*   **系统提示词（System Prompt）：** 在此处定义任务目标，例如：“你是一个友好的接待员，请根据游客偏好生成行程” [1, 12]。
*   **严格约束：** 为了防止模型幻觉，必须在提示词中明确要求模型**仅使用语义搜索结果中提供的信息** [10, 13]。
*   **数据引用：** 在任务提示词中，通过正斜杠（`/`）引用之前格式化好的“语义搜索结果”变量 [11, 12]。

### 6. 调试、发布与应用集成
*   **调试器（Debugger）：** 使用调试器查看模型的**思维链（Chain of Thought）** [8, 14]。你可以观察模型如何通过语义搜索找到即便没有显式提及关键词（如“加速度”）但含义相关（如“疯狂加速”）的结果 [2]。
*   **发布函数：** 为逻辑函数设置唯一的 API 名称并发布 [15]。
*   **Workshop 集成：** 在 Workshop 中添加 **AIP Generated Content** 组件，连接发布好的逻辑函数，并将应用内的文本输入框与函数输入关联，从而实现端到端的 AI 应用程序 [16-18]。

### RAG 流程总结
1.  **输入（Inputs）：** 接收用户查询（如偏好） [7, 15]。
2.  **检索（Retrieval）：** 通过**语义搜索块**在本体中查找最相关的对象 [3, 15]。
3.  **增强（Augmentation）：** 将对象属性转换为**字符串上下文** [11, 15]。
4.  **生成（Generation）：** **LLM 块**结合原始查询与上下文，生成逻辑严密且有据可查的回复 [15]。
