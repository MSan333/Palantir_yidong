在 Palantir Foundry 中，对 **AIP Logic 函数进行评价（Evaluations）** 是确保 AI 功能准确性且实现生产级开发的关键环节 [1]。评价流程通过对比 AI 的输出与专家定义的“事实标准”来量化模型表现 [2, 3]。

以下是 AIP Logic 评价功能的详细介绍：

### 1. 评价套件与测试用例
*   **评价套件 (Evaluation Suite)：** 评价套件是一个专门的 Foundry 资源，保存在项目文件夹中，用于统一管理和运行针对特定逻辑函数的测试 [2]。
*   **测试用例 (Test Cases)：** 每个测试用例由一组**特定输入**和对应的**预期输出（响应）**组成 [2, 3]。
*   **数据来源：** 
    *   **手动输入：** 开发者可以手动定义问题和答案，并能将在预览运行（Preview Run）中表现良好的查询直接添加为测试用例 [3, 4]。
    *   **对象支持 (Object-backed)：** 适用于大规模数据集。它允许专家通过操作（Actions）将验证过的答案回写到测试集中，构建自动化且可持续的评估工作流 [2, 5, 6]。

### 2. 评价器 (Evaluators) 的类型
评价器负责执行具体的比对逻辑，系统提供了多种内置选项 [6]：
*   **精确匹配 (Exact String Match)：** 检查输出是否与预期字符串完全一致 [6, 7]。
*   **正则表达式匹配 (Regex Match)：** 验证输出是否符合特定的文本模式 [6]。
*   **语义与格式检查：** 包括**关键词检查器 (Keyword Checker)**、**字符串长度 (String Length)** 以及计算文本差异的 **Levenshtein 距离** [6]。
*   **自定义评价：** 开发者可以编写专门的函数来处理复杂的评估逻辑，例如评估包含详细背景和解释的长篇答案 [6, 8]。

### 3. 运行评价与结果分析
*   **性能量化：** 运行评价套件后，系统会显示**匹配成功的百分比**，并详细标出哪些用例失败以及失败的具体原因（例如因为标点符号或引号导致的精确匹配失败） [7, 9]。
*   **版本对比：** 开发者在微调系统提示词（System Prompt）后，可以多次运行评价套件，并在不同运行记录之间进行**横向对比**，以验证性能是否有所提升 [7, 9]。

### 4. 数据追踪与指标监控
*   **指标仪表板 (Metrics Dashboard)：** 开发者可以通过仪表板查看聚合指标和对比报告，直观地监控逻辑函数的演进过程 [10]。
*   **运行历史记录：** 系统支持将所有输入、输出及元数据保存到 **Foundry 数据集**中，以便进行长期审计或后续分析 [4, 10]。
*   **执行权限：** 若要将历史数据存入数据集，通常需要将评价配置为**项目范围内的执行（Project-scoped execution）** [10]。

### 5. 发布与绑定
完成评估后，评价结果可以作为逻辑函数是否符合生产标准的依据 [1]。发布的 AIP Logic 函数将绑定到本体（Ontology）中，供 Workshop 应用或 AIP Agents 调用 [8]。
