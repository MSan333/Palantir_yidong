在 Palantir Foundry 的 **Pipeline Builder** 中，从 PDF 文档中提取表格并将其转换为结构化数据集是一项精细的工作流，主要依赖于**布局感知（Layout Aware）**的提取技术和正则解析。

以下是根据提供的来源整理的详细步骤：

### 1. 数据准备与媒体集上传
*   **上传 PDF：** 首先将本地 PDF 文件上传，并选择将其保存为**媒体集（Media Set）** [1, 2]。媒体集是 Foundry 存储非结构化数据（如 PDF、图片）的专用格式 [2]。
*   **转换为表格行：** 在 Pipeline Builder 中，点击媒体集节点并使用 **"Convert media set to table rows"** 转换 [2, 3]。这将生成包含文件路径、**媒体引用（Media Reference）**和资源 ID（RID）的初始表格 [3]。

### 2. 执行布局感知提取（Layout Aware Extraction）
*   **选择解析模式：** 使用 **"PDF text extraction"** 转换，并选择 **"Layout Aware"** 提取方法 [3, 4]。
*   **全量提取（Full Extract）：** 在格式选项中选择 **"Full Extract"** [4]。该模式能够识别并拆分 PDF 中的不同组件，如图像、文本、**表格（Table）**、页眉和列表 [4]。
*   **性能优化：** 由于此模型调用计算成本较高，建议开启 **"Skip recomputing rows"** 以加快预览和构建速度 [4]。

### 3. 数据平铺与过滤
*   **多层级展开（Explode）：** 提取的内容通常是一个嵌套的 2D 数组。首先执行 **"Explode array"** 获得每一页对应的行，再次执行该操作以获得每页中不同**数据块（Blocks）**对应的行 [5]。
*   **展平结构体：** 使用 **"Flatten struct"** 将提取的块信息（如块类型、内容、页码）转化为独立的列 [6]。
*   **过滤表格块：** 应用过滤器，仅保留 **"block type"** 等于 **"table"** 的行 [6, 7]。如果只需要特定页面的数据，可以同时指定页码进行过滤 [7]。

### 4. 使用正则解析（Regex）提取行列
由于提取的表格内容通常带有 HTML 标签或特定的文本结构，需要进一步解析：
*   **提取行：** 使用 **"extract all regex matches"**，通过匹配代表表格行的正则模式将内容拆分为数组，并再次执行 **"Explode"** [8, 9]。
*   **拆分列：** 使用 **"split string"** 根据列间隔的正则模式将每行拆分为数组 [9]。
*   **映射到列：** 使用 **"array elements to columns"** 插件，将数组中的每个元素按顺序映射到具体的业务列名（如“财年”、“周期”、“租金”等） [10]。

### 5. 数据清洗与类型转换
*   **批量清理：** 使用 **"apply to multiple columns"** 转换，配合 **"regex replace"** 批量去除残余的 HTML 字符或特殊符号 [11, 12]。
*   **类型转换（Cast）：** 在同一个批量处理步骤中，将清理后的字符串列转换为 **"double"** 或 **"date"** 等正确的数值类型 [12]。
*   **特定字段微调：** 针对个别仍含有垃圾字符的列（如财年中的特殊字符）单独运行一次正则替换 [13]。

### 6. 输出与部署
*   **整理架构：** 移除不再需要的中间列（如 path、block index、page 等），并按照原始 PDF 表格的逻辑顺序重新排列列 [13, 14]。
*   **发布数据集：** 将最终结果添加为 **"New dataset"** 输出并进行部署 [13, 14]。部署成功后，清理后的表格数据即可在 Ontology 或 Contour 等下游应用中使用 [15]。

这种方法不仅适用于单一表格，通过调整正则匹配模式，还可以扩展到处理分布在多个页面上的复杂表格结构 [14, 15]。
